# Azure Data Engineering â€” AdventureWorks

RepositÃ³rio tÃ©cnico do projeto de Data Engineering que implementa um pipeline completo 
(Bronze â†’ Curated â†’ Gold) utilizando **Azure Data Factory**, **ADLS Gen2** e **Azure SQL Database**.

Todas as explicaÃ§Ãµes detalhadas sobre arquitetura, decisÃµes tÃ©cnicas e fundamentos estÃ£o disponÃ­veis no artigo publicado no Medium:  
 [Construindo um Pipeline de Dados Moderno no Azure](https://medium.com/p/4d5034364e84)

---
# Tecnologias Utilizadas

- Azure Data Factory (ADF)
- Azure Data Lake Storage Gen2
- Azure SQL Database
- Mapping Data Flows (ADF)
- GitHub Integration com Azure Data Factory

---

## Arquitetura Geral

### Diagrama completo de ponta a ponta:

<img width="1042" height="601" alt="Diagrama Azure AdventureWorks" src="https://github.com/user-attachments/assets/b95e1cd6-abfa-4077-881f-458623651126" />

---

### Modelagem Relacional OLTP:

<img width="1942" height="1781" alt="Untitled" src="https://github.com/user-attachments/assets/a1492a20-7fd0-4eaa-a996-b12c7f4f21ed" />

---

### Modelagem Conceitual dos Dados

<img width="1237" height="621" alt="modelagem conceitual" src="https://github.com/user-attachments/assets/ce261e0c-9cd0-41d6-ba5c-d86513c74a12" />

---

## Estrutura de Pastas do Projeto

A organizaÃ§Ã£o a seguir reflete toda a arquitetura desenvolvida no projeto, incluindo pipelines, datasets, dataflows, consultas SQL e arquivos relacionados ao processo de engenharia de dados na Azure.

```markdown
ğŸ“ Azure Fabric
â”‚
â”œâ”€â”€ ğŸ“‚ Analise_de_dados/                # Consultas SQL e anÃ¡lises exploratÃ³rias
â”‚   â”œâ”€â”€ Consulta_1.csv
â”‚   â”œâ”€â”€ Consulta_2.csv
â”‚   â”œâ”€â”€ Consulta_3.csv
â”‚   â”œâ”€â”€ query_1.sql
â”‚   â”œâ”€â”€ query_2.sql
â”‚   â””â”€â”€ query_3.sql
â”‚
â”œâ”€â”€ ğŸ“‚ Dados_Utilizados/                # Arquivos CSV brutos usados no projeto
â”‚   â”œâ”€â”€ Person.Person.csv
â”‚   â”œâ”€â”€ Production.Product.csv
â”‚   â”œâ”€â”€ Sales.Customer.csv
â”‚   â”œâ”€â”€ Sales.SalesOrderDetail.csv
â”‚   â”œâ”€â”€ Sales.SalesOrderHeader.csv
â”‚   â””â”€â”€ Sales.SpecialOfferProduct.csv
â”‚
â”œâ”€â”€ ğŸ“‚ dataflow/                        # Dataflows do ADF (transformaÃ§Ãµes)
â”‚   â”œâ”€â”€ df_customer.json
â”‚   â”œâ”€â”€ df_person.json
â”‚   â”œâ”€â”€ df_product.json
â”‚   â”œâ”€â”€ df_salesOrderDetail.json
â”‚   â”œâ”€â”€ df_salesOrderHeader.json
â”‚   â””â”€â”€ df_specialOfferProduct.json
â”‚
â”œâ”€â”€ ğŸ“‚ dataset/                         # Datasets do ADF (raw, staging, curated)
â”‚   â”œâ”€â”€ ds_raw_customer.json
â”‚   â”œâ”€â”€ ds_raw_person.json
â”‚   â”œâ”€â”€ ds_raw_product.json
â”‚   â”œâ”€â”€ ds_raw_salesOrderDetail.json
â”‚   â”œâ”€â”€ ds_raw_salesOrderHeader.json
â”‚   â”œâ”€â”€ ds_raw_specialOfferProduct.json
â”‚   â”œâ”€â”€ ds_staging_customer.json
â”‚   â”œâ”€â”€ ds_staging_person.json
â”‚   â”œâ”€â”€ ds_staging_product.json
â”‚   â”œâ”€â”€ ds_staging_salesOrderDetail.json
â”‚   â”œâ”€â”€ ds_staging_salesOrderHeader.json
â”‚   â”œâ”€â”€ ds_staging_specialOfferProduct.json
â”‚   â”œâ”€â”€ ds_curated_customer.json
â”‚   â”œâ”€â”€ ds_curated_person.json
â”‚   â”œâ”€â”€ ds_curated_product.json
â”‚   â”œâ”€â”€ ds_curated_salesOrderDetail.json
â”‚   â”œâ”€â”€ ds_curated_salesOrderHeader.json
â”‚   â””â”€â”€ ds_curated_specialOfferProduct.json
â”‚
â”œâ”€â”€ ğŸ“‚ tabelas_sql/                     # Estrutura das tabelas utilizadas
â”‚   â”œâ”€â”€ tabelaCustomer.json
â”‚   â”œâ”€â”€ tabelaPerson.json
â”‚   â”œâ”€â”€ tabelaProduct.json
â”‚   â”œâ”€â”€ tabelaSalesOrderDetail.json
â”‚   â”œâ”€â”€ tabelaSalesOrderHeader.json
â”‚   â””â”€â”€ tabelaSpecialOfferProduct.json
â”‚
â”œâ”€â”€ ğŸ“‚ factory/                         # ConfiguraÃ§Ã£o da Data Factory exportada
â”‚   â””â”€â”€ adf-bigtech-pipeline.json
â”‚
â”œâ”€â”€ ğŸ“‚ linkedService/                   # ConexÃµes com SQL DB e Data Lake
â”‚   â”œâ”€â”€ ls_datalake_bigtech.json
â”‚   â””â”€â”€ ls_sql_bigtech.json
â”‚
â”œâ”€â”€ ğŸ“‚ pipeline/                        # Pipelines de ingestÃ£o e tratamento
â”‚   â”œâ”€â”€ CopyAllCSVToLake.json
â”‚   â””â”€â”€ PL_Curated_AdventureWorks.json
â”‚
â”œâ”€â”€ publish_config.json                 # ConfiguraÃ§Ã£o de publicaÃ§Ã£o do ADF
â””â”€â”€ README.md                           # DocumentaÃ§Ã£o do repositÃ³rio

```
---
# Estrutura e Arquitetura do Data WareHouse

A arquitetura do Data WareHouse foi organizada em quatro camadas principais, seguindo boas prÃ¡ticas de Engenharia de Dados. Abaixo estÃ£o as definiÃ§Ãµes e a relaÃ§Ã£o com os arquivos presentes no repositÃ³rio.

## Camada RAW
- Armazena os arquivos CSV exatamente como recebidos da fonte.
- Nenhuma transformaÃ§Ã£o Ã© aplicada nesta etapa.
- Ãštil como camada de auditoria e preservaÃ§Ã£o da integridade dos dados.

Arquivos:
`ds_raw_*.json`

---

## Camada STAGING
- PadronizaÃ§Ã£o de nomes de colunas.
- SeleÃ§Ã£o das colunas relevantes para o negÃ³cio.
- ConversÃ£o bÃ¡sica de tipos de dados.
- Utilizada como preparaÃ§Ã£o intermediÃ¡ria para limpeza e enriquecimento posterior.

Arquivos:
`ds_staging_*.json`  
`df_*.json`

---

## Camada CURATED
- Dados tratados, limpos e padronizados.
- Etapa utilizada como base para criaÃ§Ã£o de modelos dimensionais e anÃ¡lises.
- ContÃ©m os dados finais antes da carga no banco SQL.

Arquivos:
`ds_curated_*.json`

---

## Camada GOLD (SQL Database)
- Implementa o modelo dimensional final.
- Tabelas dimensionais (DimCustomer, DimProduct, DimPerson, etc.).
- Tabela fato central (FactInternetSales).
- Dados preparados para anÃ¡lises, dashboards e consumo por ferramentas BI.

Scripts:
`tabela*.json`

---

# Modelagem FÃ­sica (Camada GOLD)

A modelagem fÃ­sica contempla:
- CriaÃ§Ã£o de chaves substitutas (surrogate keys).
- DefiniÃ§Ã£o de relacionamentos entre dimensÃµes e fatos utilizando foreign keys.
- TipificaÃ§Ã£o adequada de colunas para otimizar desempenho no SQL.
- DefiniÃ§Ã£o do nÃ­vel de granularidade (grain) da tabela fato.

---

# Fluxo Completo do Pipeline

1. O Azure Data Factory realiza a ingestÃ£o dos arquivos CSV para a camada RAW do Data WareHouse.
2. Os dataflows realizam transformaÃ§Ãµes iniciais e enviam os dados para a camada STAGING.
3. Novos dataflows enriquecem, padronizam e entregam os dados na camada CURATED.
4. Scripts SQL sÃ£o utilizados para gerar tabelas dimensionais e fato na camada GOLD.
5. A tabela fato FactInternetSales consolida o modelo e serve como base para anÃ¡lises.

---

# Analises De Dados

1.	Escreva uma query que retorna a quantidade de linhas na tabela Sales.SalesOrderDetail pelo campo SalesOrderID, desde que tenham pelo menos trÃªs linhas de detalhes.
2.	Escreva uma query que ligue as tabelas Sales.SalesOrderDetail, Sales.SpecialOfferProduct e Production.Product e retorne os 3 produtos (Name) mais vendidos (pela soma de OrderQty), agrupados pelo nÃºmero de dias para manufatura (DaysToManufacture).
3.	Escreva uma query usando as tabelas Sales.SalesOrderHeader, Sales.SalesOrderDetail e Production.Product, de forma a obter a soma total de produtos (OrderQty) por ProductID e OrderDate.


# Recursos Utilizados para criaÃ§Ã£o do Projeto Completo

#### Para o desenvolvimento deste projeto, o ambiente permaneceu ativo durante aproximadamente um mÃªs. Devido Ã  minha rotina de trabalho em outra cidade, nÃ£o houve a necessidade de desativar ou pausar recursos ao longo desse perÃ­odo, resultando em utilizaÃ§Ã£o contÃ­nua dos serviÃ§os do Azure.

<img width="1420" height="980" alt="costanalysis_charts" src="https://github.com/user-attachments/assets/ddaf16c0-3268-4b20-a5d8-7c73180e33d5" />
